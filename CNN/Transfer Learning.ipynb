{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5852aac-7e68-4218-8195-77edd6bd3286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3726ad1b-f711-47eb-aa26-cfb5771149e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0ba00ec-a799-45ed-aad3-20d584878b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Spectrograms saved to: data/train_image\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ====== set these ======\n",
    "INPUT_ROOT  = \"data/train\"\n",
    "OUTPUT_ROOT = \"data/train_image\"\n",
    "# =======================\n",
    "\n",
    "SR = 22050\n",
    "DURATION = 3.0\n",
    "N_FFT = 2048\n",
    "HOP = 512\n",
    "N_MELS = 128\n",
    "\n",
    "def audio_to_logmel_array(audio_path):\n",
    "    y, _ = librosa.load(audio_path, sr=SR, mono=True)\n",
    "\n",
    "    target_len = int(SR * DURATION)\n",
    "    if len(y) < target_len:\n",
    "        y = np.pad(y, (0, target_len - len(y)))\n",
    "    else:\n",
    "        y = y[:target_len]\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP, n_mels=N_MELS)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # normalize to 0..1 for imsave\n",
    "    S_db -= S_db.min()\n",
    "    S_db /= (S_db.max() + 1e-9)\n",
    "    return S_db  # 2D array\n",
    "\n",
    "def convert_dataset(input_root, output_root):\n",
    "    os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "    classes = [d for d in os.listdir(input_root) if os.path.isdir(os.path.join(input_root, d))]\n",
    "    for cls in classes:\n",
    "        in_dir = os.path.join(input_root, cls)\n",
    "        out_dir = os.path.join(output_root, cls)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        for fn in os.listdir(in_dir):\n",
    "            if not fn.lower().endswith((\".au\", \".wav\", \".mp3\", \".flac\", \".ogg\", \".m4a\")):\n",
    "                continue\n",
    "\n",
    "            in_path = os.path.join(in_dir, fn)\n",
    "            out_path = os.path.join(out_dir, os.path.splitext(fn)[0] + \".png\")\n",
    "\n",
    "            try:\n",
    "                S_norm = audio_to_logmel_array(in_path)\n",
    "                plt.imsave(out_path, S_norm, cmap=\"magma\")  # saves RGB\n",
    "            except Exception as e:\n",
    "                print(\"Failed:\", in_path, \"->\", e)\n",
    "\n",
    "    print(\"Done! Spectrograms saved to:\", output_root)\n",
    "\n",
    "convert_dataset(INPUT_ROOT, OUTPUT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0517da5a-0b84-4b98-87fd-ae0d349d84fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Test spectrograms in: data/test_image\n"
     ]
    }
   ],
   "source": [
    "INPUT_TEST  = \"data/test\"\n",
    "OUTPUT_TEST = \"data/test_image\"   # one folder (no labels)\n",
    "\n",
    "SR = 22050\n",
    "DURATION = 3.0\n",
    "N_FFT = 2048\n",
    "HOP = 512\n",
    "N_MELS = 128\n",
    "\n",
    "def audio_to_logmel_norm(audio_path):\n",
    "    y, _ = librosa.load(audio_path, sr=SR, mono=True)\n",
    "\n",
    "    target_len = int(SR * DURATION)\n",
    "    if len(y) < target_len:\n",
    "        y = np.pad(y, (0, target_len - len(y)))\n",
    "    else:\n",
    "        y = y[:target_len]\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=SR, n_fft=N_FFT, hop_length=HOP, n_mels=N_MELS)\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    # normalize to 0..1 so imsave can colorize nicely\n",
    "    S_db -= S_db.min()\n",
    "    S_db /= (S_db.max() + 1e-9)\n",
    "    return S_db\n",
    "\n",
    "os.makedirs(OUTPUT_TEST, exist_ok=True)\n",
    "\n",
    "for fn in sorted(os.listdir(INPUT_TEST)):\n",
    "    if not fn.lower().endswith((\".au\", \".wav\", \".mp3\", \".flac\", \".ogg\", \".m4a\")):\n",
    "        continue\n",
    "\n",
    "    in_path = os.path.join(INPUT_TEST, fn)\n",
    "    out_path = os.path.join(OUTPUT_TEST, os.path.splitext(fn)[0] + \".png\")\n",
    "\n",
    "    try:\n",
    "        S_norm = audio_to_logmel_norm(in_path)\n",
    "        plt.imsave(out_path, S_norm, cmap=\"magma\")  # saves RGB image\n",
    "    except Exception as e:\n",
    "        print(\"Failed:\", fn, \"->\", e)\n",
    "\n",
    "print(\"Done! Test spectrograms in:\", OUTPUT_TEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bc50fc9-989b-44a3-8c09-3997742e7e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resnet_pytorch</strong> at: <a href='https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project/runs/5og2firq' target=\"_blank\">https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project/runs/5og2firq</a><br> View project at: <a href='https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project' target=\"_blank\">https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251207_210844-5og2firq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kritimbastola/Desktop/CNN/wandb/run-20251207_211036-czcxllja</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project/runs/czcxllja' target=\"_blank\">resnet_pytorch</a></strong> to <a href='https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project' target=\"_blank\">https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project/runs/czcxllja' target=\"_blank\">https://wandb.ai/kritimbastola2005-university-of-new-mexico-press/resnet50_project/runs/czcxllja</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project='resnet50_project', name='resnet_pytorch')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d331989-31a9-4644-936a-dc967e27a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "\ttransforms.Resize((224)),\n",
    "\ttransforms.ToTensor(),\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='data/train_image', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True,  pin_memory=False, num_workers=0)\n",
    "test_dataset = datasets.CIFAR10(root='data/test_image', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False,  pin_memory=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b75b0bd8-d41d-4f4c-9f5a-0e44ffa07898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "\n",
    "        # resnet50 final layer input features = 2048\n",
    "        in_features = self.resnet.fc.in_features\n",
    "\n",
    "        # remove the original classifier head\n",
    "        self.resnet.fc = nn.Identity()\n",
    "\n",
    "        # your custom head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)        # (B, 2048)\n",
    "        x = self.classifier(x)    # (B, num_classes)\n",
    "        return x\n",
    "\n",
    "model = CustomResNet(num_classes=10).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32b7578e-82e3-460a-a45d-bc96524efcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 0 minibatch 782 train loss 1.576: 100%|█| 782/782 [2:16:03<00:0\n",
      "Processing epoch 1 minibatch 782 train loss 0.268: 100%|█| 782/782 [2:14:30<00:0\n",
      "Processing epoch 2 minibatch 782 train loss 0.139: 100%|█| 782/782 [2:17:43<00:0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    data_bar = tqdm(train_loader)\n",
    "    i = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for data in data_bar:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        data_bar.set_description(\n",
    "            \"Processing epoch {:d} minibatch {:d} train loss {:.3f}\".format(\n",
    "                epoch, i + 1, running_loss / (i + 1)\n",
    "            )\n",
    "        )\n",
    "        i += 1\n",
    "\n",
    "    # Log metrics to wandb\n",
    "    wandb.log({'epoch': epoch + 1, 'train_loss': running_loss / len(train_loader)})\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "782bbb92-7e9c-411b-8caf-b8c275172d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "minibatch 782 test accuracy 96.80%:   0%|             | 0/157 [1:23:13<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 96.80 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "\tpbar = tqdm(test_loader)\n",
    "\tfor data in data_bar:\n",
    "\t\timages, labels = data\n",
    "\t\timages = images.to(device)\n",
    "\t\tlabels = labels.to(device)\n",
    "\t\toutputs = model(images)\n",
    "\t\t_, predicted = torch.max(outputs.data, 1)\n",
    "\t\ttotal += labels.size(0)\n",
    "\t\tcorrect += (predicted == labels).sum().item()\n",
    "\t\tpbar.set_description(\"minibatch {:d} test accuracy {:4.2f}%\".format(i+1,100.0*correct/total))\n",
    "\t\ti += 1\n",
    "\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %4.2f %%' % (100.0 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db1421bd-c1e0-44c3-8199-88f89fddc32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/Caskroom/miniconda/base/envs/tf312/lib/python3.12/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/tf312/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/tf312/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/tf312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [pandas]2m1/2\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.3 pytz-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25a4c8e8-e89e-4249-93be-8923a2faae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test.00596.au</td>\n",
       "      <td>reggae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test.02436.au</td>\n",
       "      <td>reggae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test.02930.au</td>\n",
       "      <td>reggae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test.03364.au</td>\n",
       "      <td>disco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test.03550.au</td>\n",
       "      <td>country</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id    class\n",
       "0  test.00596.au   reggae\n",
       "1  test.02436.au   reggae\n",
       "2  test.02930.au   reggae\n",
       "3  test.03364.au    disco\n",
       "4  test.03550.au  country"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "# ---- paths ----\n",
    "LIST_PATH = \"data/test/list_test.txt\"          \n",
    "IMG_DIR   = \"data/test_image\"          \n",
    "OUT_CSV   = \"submission.csv\"\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Must match the folder order\n",
    "class_names = [\"blues\",\"classical\",\"country\",\"disco\",\"hiphop\",\"jazz\",\"metal\",\"pop\",\"reggae\",\"rock\"]\n",
    "\n",
    "# 1) read filenames\n",
    "with open(LIST_PATH, \"r\") as f:\n",
    "    filenames = [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "# 2) dataset that loads the corresponding spectrogram png for each id\n",
    "class TestSpecDataset(Dataset):\n",
    "    def __init__(self, filenames, img_dir, transform):\n",
    "        self.filenames = filenames\n",
    "        self.img_paths = [\n",
    "            os.path.join(img_dir, os.path.splitext(fn)[0] + \".png\")\n",
    "            for fn in filenames\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
    "        img = self.transform(img)\n",
    "        return img, self.filenames[idx]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_ds = TestSpecDataset(filenames, IMG_DIR, transform)\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# 3) predict -> preds_label\n",
    "model.eval()\n",
    "preds_label = []\n",
    "ids_out = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, batch_ids in test_loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)                 # (B, 10)\n",
    "        pred_idx = logits.argmax(dim=1).cpu().tolist()\n",
    "        preds_label.extend([class_names[i] for i in pred_idx])\n",
    "        ids_out.extend(list(batch_ids))\n",
    "\n",
    "# 4) write submission.csv\n",
    "submission = pd.DataFrame({\"id\": ids_out, \"class\": preds_label})\n",
    "submission.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "print(\"Saved:\", OUT_CSV)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a054da6-b04c-48c6-90de-36e10779d8cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
