{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d92d7e5-4089-427e-9323-3095b8ad3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_mean_cols     = [f\"mfcc_mean{i}\"     for i in range(1, 13+1)]\n",
    "mfcc_stds_cols     = [f\"mfcc_stds{i}\"     for i in range(1, 13+1)]\n",
    "mfcc_q25_cols      = [f\"mfcc_q25{i}\"     for i in range(1, 13+1)]\n",
    "mfcc_q50_cols      = [f\"mfcc_q50{i}\"     for i in range(1, 13+1)]\n",
    "mfcc_q75_cols      = [f\"mfcc_q75{i}\"     for i in range(1, 13+1)]\n",
    "\n",
    "delta_mfcc_mean_cols     = [f\"delta_mfcc_mean{i}\"     for i in range(1, 13+1)]\n",
    "delta_mfcc_stds_cols     = [f\"delta_mfcc_stds{i}\"     for i in range(1, 13+1)]\n",
    "delta_mfcc_q25_cols      = [f\"delta_mfcc_q25{i}\"     for i in range(1, 13+1)]\n",
    "delta_mfcc_q50_cols      = [f\"delta_mfcc_q50{i}\"     for i in range(1, 13+1)]\n",
    "delta_mfcc_q75_cols      = [f\"delta_mfcc_q75{i}\"     for i in range(1, 13+1)]\n",
    "\n",
    "delta_delta_mfcc_mean_cols     = [f\"delta_delta_mfcc_mean{i}\"     for i in range(1, 13+1)]\n",
    "delta_delta_mfcc_stds_cols     = [f\"delta_delta_mfcc_stds{i}\"     for i in range(1, 13+1)]\n",
    "delta_delta_mfcc_q25_cols      = [f\"delta_delta_mfcc_q25{i}\"     for i in range(1, 13+1)]\n",
    "delta_delta_mfcc_q50_cols      = [f\"delta_delta_mfcc_q50{i}\"     for i in range(1, 13+1)]\n",
    "delta_delta_mfcc_q75_cols      = [f\"delta_delta_mfcc_q75{i}\"     for i in range(1, 13+1)]\n",
    "\n",
    "stft_mean_cols    = [f\"stft_mean{i}\"     for i in range(1, 1025+1)]\n",
    "stft_stds_cols    = [f\"stft_stds{i}\"     for i in range(1, 1025+1)]\n",
    "stft_q25_cols     = [f\"stft_q25{i}\"     for i in range(1, 1025+1)]\n",
    "stft_q50_cols     = [f\"stft_q50{i}\"     for i in range(1, 1025+1)]\n",
    "stft_q75_cols     = [f\"stft_q75{i}\"     for i in range(1, 1025+1)]\n",
    "\n",
    "\n",
    "chroma_mean_cols  = [f\"chroma_mean{i}\"   for i in range(1, 12+1)]\n",
    "chroma_stds_cols  = [f\"chroma_stds{i}\"   for i in range(1, 12+1)]\n",
    "chroma_q25_cols   = [f\"chroma_q25{i}\"   for i in range(1, 12+1)]\n",
    "chroma_q50_cols   = [f\"chroma_q50{i}\"   for i in range(1, 12+1)]\n",
    "chroma_q75_cols   = [f\"chroma_q75{i}\"   for i in range(1, 12+1)]\n",
    "\n",
    "\n",
    "contrast_mean_cols = [f\"contrast_mean{i}\" for i in range(1, 7+1)]\n",
    "contrast_stds_cols = [f\"contrast_stds{i}\" for i in range(1, 7+1)]\n",
    "contrast_q25_cols = [f\"contrast_q25{i}\" for i in range(1, 7+1)]\n",
    "contrast_q50_cols = [f\"contrast_q50{i}\" for i in range(1, 7+1)]\n",
    "contrast_q75_cols = [f\"contrast_q75{i}\" for i in range(1, 7+1)]\n",
    "\n",
    "\n",
    "FEATURE_COLS  = mfcc_mean_cols + mfcc_stds_cols + mfcc_q25_cols + mfcc_q50_cols + mfcc_q75_cols + delta_mfcc_mean_cols + delta_mfcc_stds_cols + delta_mfcc_q25_cols + delta_mfcc_q50_cols + delta_mfcc_q75_cols+ delta_delta_mfcc_mean_cols + delta_delta_mfcc_stds_cols + delta_delta_mfcc_q25_cols + delta_delta_mfcc_q50_cols + delta_delta_mfcc_q75_cols +stft_mean_cols +  stft_stds_cols + stft_q25_cols + stft_q50_cols + stft_q75_cols +chroma_mean_cols + chroma_stds_cols + chroma_q25_cols + chroma_q50_cols + chroma_q75_cols +contrast_mean_cols + contrast_stds_cols + contrast_q25_cols + contrast_q50_cols + contrast_q75_cols\n",
    "ALL_COLS      = FEATURE_COLS + [\"genre\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5fc81fe-b95b-4002-b447-e9db13532b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Statements\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "## MLP CLASS\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.2):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.cost = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"Final_Dataset_Logistic_regression.csv\")\n",
    "\n",
    "genre_to_idx = {\n",
    "    'pop': 0, 'metal': 1, 'disco': 2, 'blues': 3, 'reggae': 4,\n",
    "    'classical': 5, 'rock': 6, 'hiphop': 7, 'country': 8, 'jazz': 9\n",
    "}\n",
    "\n",
    "# 1) Features: all columns except the last  -> (900, 5415)\n",
    "X = df.iloc[:, :-1].to_numpy(dtype=float)\n",
    "\n",
    "lab = df.iloc[:, -1].astype(str).str.strip().str.lower()\n",
    "y = lab.map(genre_to_idx).to_numpy()\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train, X_test_val, y_train, y_test_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test_val = scaler.transform(X_test_val)\n",
    "# X_test, X_val, y_test, y_val = train_test_split(X_test_val, y_test_val, test_size=0.5, random_state=42, stratify=y_test_val)\n",
    "\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train_t = torch.tensor(X_tr, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_tr, dtype=torch.long)\n",
    "X_val_t   = torch.tensor(X_va, dtype=torch.float32)\n",
    "y_val_t   = torch.tensor(y_va, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset   = TensorDataset(X_val_t, y_val_t)\n",
    "\n",
    "# Create PyTorch datasets and dataloaders\n",
    "# train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "# val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "# test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dedb450f-037b-44d7-91bb-a5394d3a8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(batch_size, num_epochs, hidden_size)\n",
    "# ## Batch size = 32 hidden_size = 64, num_epochs = 10\n",
    "#     batch_size=batch_size\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "#     # Initialize MLP model\n",
    "#     input_size = X_train.shape[1]\n",
    "#     hidden_size = hidden_size\n",
    "#     output_size = len(np.unique(y_train))  # Number of classes\n",
    "    \n",
    "#     model = MLP(input_size, hidden_size, output_size)\n",
    "#     print(model)\n",
    "    \n",
    "#     # Define optimizer and loss function\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    \n",
    "#     # Training loop\n",
    "#     num_epochs = num_epochs\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Training\n",
    "#         model.train()\n",
    "#         train_loss = 0.0\n",
    "#         correct_train = 0\n",
    "#         total_train = 0\n",
    "#         for inputs, targets in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = model.cost(outputs, targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             total_train += targets.size(0)\n",
    "#             correct_train += (predicted == targets).sum().item()\n",
    "    \n",
    "#         train_accuracy = correct_train / total_train\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss / len(train_loader):.4f}, Train Accuracy: {100 * train_accuracy:.2f}%\")\n",
    "    \n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         val_loss = 0.0\n",
    "#         correct_val = 0\n",
    "#         total_val = 0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, targets in val_loader:\n",
    "#                 outputs = model(inputs)\n",
    "#                 loss = model.cost(outputs, targets)\n",
    "#                 val_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 total_val += targets.size(0)\n",
    "#                 correct_val += (predicted == targets).sum().item()\n",
    "    \n",
    "#         val_accuracy = correct_val / total_val\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss / len(val_loader):.4f}, Validation Accuracy: {100 * val_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf0a46c4-7495-49e3-9e8d-0eb753f4997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def train_and_eval(h, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    device = get_device()\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=h[\"batch_size\"], shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=h[\"batch_size\"], shuffle=False)\n",
    "\n",
    "    input_size = X_train_t.shape[1]\n",
    "    output_size = int(torch.unique(y_train_t).numel())\n",
    "\n",
    "    model = MLP(input_size, h[\"hidden_size\"], output_size, dropout_rate=h[\"dropout\"]).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=h[\"lr\"],\n",
    "        weight_decay=h[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    for epoch in range(h[\"epochs\"]):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = model.cost(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb).argmax(dim=1)\n",
    "            correct += (pred == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "    val_acc = correct / max(total, 1)\n",
    "    return val_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8a9801c-56cc-41a8-a074-06330947ba92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/25  val_acc=0.4494  config={'hidden_size': 64, 'dropout': 0.33557781680810966, 'lr': 0.0017224205375144482, 'weight_decay': 3.794437488678893e-05, 'batch_size': 64, 'epochs': 20}\n",
      "Trial 2/25  val_acc=0.5225  config={'hidden_size': 64, 'dropout': 0.33557781680810966, 'lr': 0.0017224205375144482, 'weight_decay': 3.794437488678893e-05, 'batch_size': 64, 'epochs': 20}\n",
      "Trial 3/25  val_acc=0.1742  config={'hidden_size': 512, 'dropout': 0.2762700936595633, 'lr': 0.07211956091117917, 'weight_decay': 0.0008981375289786563, 'batch_size': 128, 'epochs': 30}\n",
      "Trial 4/25  val_acc=0.4607  config={'hidden_size': 64, 'dropout': 0.19877871750290282, 'lr': 0.0024423264773299814, 'weight_decay': 8.523537174786087e-06, 'batch_size': 128, 'epochs': 15}\n",
      "Trial 5/25  val_acc=0.1236  config={'hidden_size': 512, 'dropout': 0.12954700803091845, 'lr': 0.02681681408556237, 'weight_decay': 1.451042509852497e-06, 'batch_size': 16, 'epochs': 15}\n",
      "Trial 6/25  val_acc=0.1011  config={'hidden_size': 128, 'dropout': 0.06217070753983539, 'lr': 0.02228461989985422, 'weight_decay': 1.9428932032734277e-05, 'batch_size': 16, 'epochs': 10}\n",
      "Trial 7/25  val_acc=0.1011  config={'hidden_size': 64, 'dropout': 0.16413266708517926, 'lr': 0.03553898498778913, 'weight_decay': 8.884564391986325e-05, 'batch_size': 16, 'epochs': 10}\n",
      "Trial 8/25  val_acc=0.6348  config={'hidden_size': 256, 'dropout': 0.34728347865606346, 'lr': 0.002684155401530714, 'weight_decay': 3.146166105465005e-05, 'batch_size': 64, 'epochs': 30}\n",
      "Trial 9/25  val_acc=0.5112  config={'hidden_size': 256, 'dropout': 0.4955290770336273, 'lr': 0.004060976776295742, 'weight_decay': 1.6963428605718182e-06, 'batch_size': 32, 'epochs': 20}\n",
      "Trial 10/25  val_acc=0.5169  config={'hidden_size': 512, 'dropout': 0.4536093176692263, 'lr': 0.003388907098488417, 'weight_decay': 6.847601753827784e-05, 'batch_size': 16, 'epochs': 20}\n",
      "Trial 11/25  val_acc=0.5730  config={'hidden_size': 256, 'dropout': 0.31769028116548953, 'lr': 0.003123602157787537, 'weight_decay': 3.214463391522846e-05, 'batch_size': 32, 'epochs': 10}\n",
      "Trial 12/25  val_acc=0.5281  config={'hidden_size': 64, 'dropout': 0.406426637092439, 'lr': 0.0009594539642463171, 'weight_decay': 0.000735869265835909, 'batch_size': 16, 'epochs': 10}\n",
      "Trial 13/25  val_acc=0.7079  config={'hidden_size': 64, 'dropout': 0.060378891114405364, 'lr': 0.0008042079328432119, 'weight_decay': 0.0003645183327088353, 'batch_size': 64, 'epochs': 20}\n",
      "Trial 14/25  val_acc=0.5169  config={'hidden_size': 64, 'dropout': 0.3537877710174941, 'lr': 0.0008745054957302468, 'weight_decay': 0.00035504169498215473, 'batch_size': 16, 'epochs': 30}\n",
      "Trial 15/25  val_acc=0.6966  config={'hidden_size': 128, 'dropout': 0.06159647581690397, 'lr': 0.0002414875246950737, 'weight_decay': 0.0005168127260653841, 'batch_size': 128, 'epochs': 15}\n",
      "Trial 16/25  val_acc=0.1742  config={'hidden_size': 256, 'dropout': 0.06320938791612263, 'lr': 0.023739055287542835, 'weight_decay': 1.604293225479075e-06, 'batch_size': 64, 'epochs': 10}\n",
      "Trial 17/25  val_acc=0.6404  config={'hidden_size': 256, 'dropout': 0.1457834754242539, 'lr': 0.0013012762352740475, 'weight_decay': 4.7366421320971404e-05, 'batch_size': 64, 'epochs': 15}\n",
      "Trial 18/25  val_acc=0.1011  config={'hidden_size': 128, 'dropout': 0.28844254221285653, 'lr': 0.09398498155967958, 'weight_decay': 3.318386557059563e-05, 'batch_size': 64, 'epochs': 30}\n",
      "Trial 19/25  val_acc=0.7303  config={'hidden_size': 256, 'dropout': 0.3951312426452827, 'lr': 0.00013110471556195203, 'weight_decay': 3.3461535650045624e-06, 'batch_size': 32, 'epochs': 15}\n",
      "Trial 20/25  val_acc=0.7135  config={'hidden_size': 64, 'dropout': 0.05704832665301074, 'lr': 0.0003885099585400804, 'weight_decay': 8.491775237410847e-06, 'batch_size': 32, 'epochs': 30}\n",
      "Trial 21/25  val_acc=0.1011  config={'hidden_size': 256, 'dropout': 0.08394079468336402, 'lr': 0.052193574349721515, 'weight_decay': 1.5219252218451406e-06, 'batch_size': 16, 'epochs': 10}\n",
      "Trial 22/25  val_acc=0.1011  config={'hidden_size': 512, 'dropout': 0.18277475428938417, 'lr': 0.09786766767482455, 'weight_decay': 4.972504392796239e-05, 'batch_size': 16, 'epochs': 10}\n",
      "Trial 23/25  val_acc=0.1798  config={'hidden_size': 256, 'dropout': 0.39561133910256463, 'lr': 0.09410836175371581, 'weight_decay': 0.0002651723531163512, 'batch_size': 128, 'epochs': 30}\n",
      "Trial 24/25  val_acc=0.5000  config={'hidden_size': 512, 'dropout': 0.3981819029979782, 'lr': 0.002794850742086515, 'weight_decay': 0.00032302521767323247, 'batch_size': 16, 'epochs': 30}\n",
      "Trial 25/25  val_acc=0.6573  config={'hidden_size': 256, 'dropout': 0.3564776602616136, 'lr': 0.0019293578151123357, 'weight_decay': 0.00011821009465295127, 'batch_size': 32, 'epochs': 30}\n",
      "\n",
      "BEST VAL_ACC: 0.7303370786516854\n",
      "BEST CONFIG: {'hidden_size': 256, 'dropout': 0.3951312426452827, 'lr': 0.00013110471556195203, 'weight_decay': 3.3461535650045624e-06, 'batch_size': 32, 'epochs': 15}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>trial</th>\n",
       "      <th>val_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>256</td>\n",
       "      <td>0.395131</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>0.730337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>64</td>\n",
       "      <td>0.057048</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>0.713483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>64</td>\n",
       "      <td>0.060379</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>0.707865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>128</td>\n",
       "      <td>0.061596</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>128</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.696629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>256</td>\n",
       "      <td>0.356478</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>0.657303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>256</td>\n",
       "      <td>0.145783</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>64</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>0.640449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>256</td>\n",
       "      <td>0.347283</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>0.634831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256</td>\n",
       "      <td>0.317690</td>\n",
       "      <td>0.003124</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0.573034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>64</td>\n",
       "      <td>0.406427</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>0.528090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>0.335578</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.522472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    hidden_size   dropout        lr  weight_decay  batch_size  epochs  trial  \\\n",
       "18          256  0.395131  0.000131      0.000003          32      15     19   \n",
       "19           64  0.057048  0.000389      0.000008          32      30     20   \n",
       "12           64  0.060379  0.000804      0.000365          64      20     13   \n",
       "14          128  0.061596  0.000241      0.000517         128      15     15   \n",
       "24          256  0.356478  0.001929      0.000118          32      30     25   \n",
       "16          256  0.145783  0.001301      0.000047          64      15     17   \n",
       "7           256  0.347283  0.002684      0.000031          64      30      8   \n",
       "10          256  0.317690  0.003124      0.000032          32      10     11   \n",
       "11           64  0.406427  0.000959      0.000736          16      10     12   \n",
       "1            64  0.335578  0.001722      0.000038          64      20      2   \n",
       "\n",
       "     val_acc  \n",
       "18  0.730337  \n",
       "19  0.713483  \n",
       "12  0.707865  \n",
       "14  0.696629  \n",
       "24  0.657303  \n",
       "16  0.640449  \n",
       "7   0.634831  \n",
       "10  0.573034  \n",
       "11  0.528090  \n",
       "1   0.522472  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_uniform(a, b):\n",
    "    # samples from [10^a, 10^b]\n",
    "    return 10 ** np.random.uniform(a, b)\n",
    "\n",
    "def sample_config():\n",
    "    return {\n",
    "        \"hidden_size\": int(np.random.choice([64, 128, 256, 512])),\n",
    "        \"dropout\": float(np.random.uniform(0.0, 0.5)),\n",
    "        \"lr\": float(log_uniform(-4, -1)),          # 1e-4 .. 1e-1\n",
    "        \"weight_decay\": float(log_uniform(-6, -3)),# 1e-6 .. 1e-3\n",
    "        \"batch_size\": int(np.random.choice([16, 32, 64, 128])),\n",
    "        \"epochs\": int(np.random.choice([10, 15, 20, 30])),\n",
    "    }\n",
    "\n",
    "def random_search(n_trials=25, base_seed=100):\n",
    "    best = {\"val_acc\": -1, \"config\": None, \"model\": None}\n",
    "    rows = []\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        h = sample_config()\n",
    "        val_acc, model = train_and_eval(h, seed=base_seed + t)\n",
    "\n",
    "        rows.append({**h, \"trial\": t+1, \"val_acc\": val_acc})\n",
    "        print(f\"Trial {t+1}/{n_trials}  val_acc={val_acc:.4f}  config={h}\")\n",
    "\n",
    "        if val_acc > best[\"val_acc\"]:\n",
    "            best = {\"val_acc\": val_acc, \"config\": h, \"model\": model}\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values(\"val_acc\", ascending=False)\n",
    "    return best, df\n",
    "\n",
    "best, results_df = random_search(n_trials=25)\n",
    "print(\"\\nBEST VAL_ACC:\", best[\"val_acc\"])\n",
    "print(\"BEST CONFIG:\", best[\"config\"])\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "112406c5-e950-4c7e-b758-71d51ffce25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize network architecture\n",
    "\n",
    "# from torchview import draw_graph\n",
    "\n",
    "# device = 'cpu'\n",
    "# # device='meta' -> no memory is consumed for visualization\n",
    "# model_graph = draw_graph(model, input_size=(batch_size, input_size), device='cpu')\n",
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2d8924d-c704-41e6-93b7-948a1e5b19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # visualize the propagation of gradients\n",
    "\n",
    "# from torchviz import make_dot\n",
    "\n",
    "# X,Y = next(iter(test_loader))\n",
    "# device = next(model.parameters()).device\n",
    "# X = X.to(device)\n",
    "# yhat = model(X)\n",
    "\n",
    "# make_dot(yhat.mean(), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b35cde16-63d0-4015-802c-3976f142bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(\"test_features.csv\")\n",
    "\n",
    "\n",
    "# # 0) put model in eval mode\n",
    "# model.eval()\n",
    "\n",
    "# # 1) take your new input features (one sample or many)\n",
    "# #    X_new should be shape (N, d)  (same d as training)\n",
    "# #    and preprocessed the same way as training\n",
    "# X_new = ...  # numpy array\n",
    "\n",
    "# # 2) apply the SAME preprocessing you used in training\n",
    "# #    (ex: scaler)\n",
    "# X_new = scaler.transform(X_new)\n",
    "\n",
    "# # 3) convert to torch tensor\n",
    "# X_new_t = torch.tensor(X_new, dtype=torch.float32)\n",
    "\n",
    "# # 4) forward pass without gradients\n",
    "# with torch.no_grad():\n",
    "#     logits = model(X_new_t)              # (N, num_classes)\n",
    "#     preds  = logits.argmax(dim=1)        # (N,)\n",
    "\n",
    "# idx_to_genre = {\n",
    "#     0:'pop',1:'metal',2:'disco',3:'blues',4:'reggae',\n",
    "#     5:'classical',6:'rock',7:'hiphop',8:'country',9:'jazz'\n",
    "# }\n",
    "\n",
    "# labels = [idx_to_genre[i] for i in preds]\n",
    "# print(labels)   # or use labels to make your output csv    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e20139e-00fb-4e9d-ba13-df626cb6733b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (100, 5415)\n",
      "Expected d: 5415\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m X_test = scaler.transform(X_test)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 4) predict\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mmodel\u001b[49m.eval()\n\u001b[32m     25\u001b[39m X_test_t = torch.tensor(X_test, dtype=torch.float32)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 0) mappings\n",
    "genre_to_idx = {\n",
    "    'pop': 0, 'metal': 1, 'disco': 2, 'blues': 3, 'reggae': 4,\n",
    "    'classical': 5, 'rock': 6, 'hiphop': 7, 'country': 8, 'jazz': 9\n",
    "}\n",
    "idx_to_genre = {v:k for k,v in genre_to_idx.items()}\n",
    "\n",
    "# 1) load test csv (no genre column)\n",
    "df_test = pd.read_csv(\"test_features.csv\")\n",
    "\n",
    "## Check if the test feature and Final_Dataset_for_logistic have same column\n",
    "\n",
    "# 2) align columns EXACTLY like training\n",
    "df_test_aligned = df_test.reindex(columns=FEATURE_COLS).fillna(0.0)\n",
    "X_test = df_test_aligned.to_numpy(dtype=np.float32)\n",
    "\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Expected d:\", len(FEATURE_COLS))  # should match X_test.shape[1]\n",
    "\n",
    "# 3) scale using training scaler\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 4) predict\n",
    "model.eval()\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test_t)\n",
    "    preds_idx = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "preds_label = [idx_to_genre[i] for i in preds_idx]\n",
    "\n",
    "print(preds_label[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e47a4d11-4291-4b72-a5ca-6b9367222fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We have all the names in data/test/list_test.txt\n",
    "## We have all the results here above\n",
    "\n",
    "filenames = []\n",
    "\n",
    "with open(\"data/test/list_test.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        file = line.strip()\n",
    "        filenames.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b99cbceb-fe2e-48e8-a8e1-74c738953206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\": filenames,\n",
    "    \"class\" : preds_label\n",
    "})\n",
    "\n",
    "df.to_csv(\"submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
